{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3618873f-c956-4c04-ac2d-75e2615a90ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually set the start date and end date\n",
    "start_date = datetime(2021, 2, 1)\n",
    "end_date = datetime(2021, 2, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0daca7f-f5db-4433-9391-356b0d16e0dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------------+--------+------------+------------------------+--------------------------------+----------+------------+------------+\n",
      "|    | Date                   | Time   | Currency   | Impact                 | Event                          | Actual   | Forecast   | Previous   |\n",
      "+====+========================+========+============+========================+================================+==========+============+============+\n",
      "|  0 | Monday, 01 February    | 4:00pm | USD        | High Impact Expected   | ISM Manufacturing PMI          | 58.7     | 60.0       | 60.7       |\n",
      "+----+------------------------+--------+------------+------------------------+--------------------------------+----------+------------+------------+\n",
      "|  1 | Wednesday, 03 February | 2:15pm | USD        | Medium Impact Expected | ADP Non-Farm Employment Change | 174K     | 48K        | -78K       |\n",
      "+----+------------------------+--------+------------+------------------------+--------------------------------+----------+------------+------------+\n",
      "|  2 | Wednesday, 03 February | 4:00pm | USD        | Medium Impact Expected | ISM Services PMI               | 58.7     | 56.7       | 57.2       |\n",
      "+----+------------------------+--------+------------+------------------------+--------------------------------+----------+------------+------------+\n",
      "|  3 | Wednesday, 03 February | 4:30pm | USD        | Medium Impact Expected | Crude Oil Inventories          | -1.0M    | -0.6M      | -9.9M      |\n",
      "+----+------------------------+--------+------------+------------------------+--------------------------------+----------+------------+------------+\n",
      "|  4 | Thursday, 04 February  | 2:30pm | USD        | Medium Impact Expected | Unemployment Claims            | 779K     | 828K       | 812K       |\n",
      "+----+------------------------+--------+------------+------------------------+--------------------------------+----------+------------+------------+\n",
      "|  5 | Friday, 05 February    | 2:30pm | USD        | High Impact Expected   | Average Hourly Earnings m/m    | 0.2%     | 0.3%       | 1.0%       |\n",
      "+----+------------------------+--------+------------+------------------------+--------------------------------+----------+------------+------------+\n",
      "|  6 | Friday, 05 February    | 2:30pm | USD        | High Impact Expected   | Non-Farm Employment Change     | 49K      | 85K        | -227K      |\n",
      "+----+------------------------+--------+------------+------------------------+--------------------------------+----------+------------+------------+\n",
      "|  7 | Friday, 05 February    | 2:30pm | USD        | High Impact Expected   | Unemployment Rate              | 6.3%     | 6.7%       | 6.7%       |\n",
      "+----+------------------------+--------+------------+------------------------+--------------------------------+----------+------------+------------+\n"
     ]
    }
   ],
   "source": [
    "# Initialize Redis client\n",
    "redis_client = redis.StrictRedis(host='localhost', port=6379, db=0)\n",
    "\n",
    "# Generate the list of dates in the format needed for the URL\n",
    "def generate_date_list(start_date, end_date):\n",
    "    return [(start_date + timedelta(days=x)).strftime('%b%d.%Y').lower() for x in range((end_date - start_date).days + 1)]\n",
    "\n",
    "# Format date to \"Day, Number Month\"\n",
    "def format_date_for_table(date_str):\n",
    "    date_obj = datetime.strptime(date_str, '%b%d.%Y')\n",
    "    return date_obj.strftime('%A, %d %B')\n",
    "\n",
    "# Setup Chrome options for Selenium\n",
    "chrome_options = Options()\n",
    "chrome_options.binary_location = chrome_path\n",
    "chrome_options.add_argument(\"--disable-gpu\")\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "# Initialize undetected ChromeDriver\n",
    "driver = uc.Chrome(options=chrome_options, use_subprocess=True)\n",
    "\n",
    "# Prepare to store all collected data\n",
    "all_events = []\n",
    "\n",
    "# Function to fetch data for the given date range\n",
    "def fetch_data_for_date_range(start_date, end_date):\n",
    "    date_list = generate_date_list(start_date, end_date)\n",
    "    \n",
    "    for date in date_list:\n",
    "        # Check if data for this date is in the Redis cache\n",
    "        cached_data = redis_client.get(date)\n",
    "        if cached_data:\n",
    "            print(f\"Loading data from cache...\")\n",
    "            events = pickle.loads(cached_data)\n",
    "            all_events.extend(events)\n",
    "            continue  # Skip to the next date\n",
    "        \n",
    "        # Generate the URL for the current date\n",
    "        url = f\"https://www.forexfactory.com/calendar?day={date}\"\n",
    "        \n",
    "        # Navigate to the page\n",
    "        driver.get(url)\n",
    "        \n",
    "        # Wait for the page to fully load\n",
    "        time.sleep(1)\n",
    "        \n",
    "        # Locate the table or section containing the calendar events\n",
    "        try:\n",
    "            calendar_table = driver.find_element(By.CSS_SELECTOR, 'table.calendar__table')\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading data for {date}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        # Initialize variable to keep track of the current time\n",
    "        current_time = None\n",
    "        formatted_date = format_date_for_table(date)  # Format the date for the table\n",
    "        events = []  # Store events for this date\n",
    "        \n",
    "        # Loop through each row in the table\n",
    "        for row in calendar_table.find_elements(By.CSS_SELECTOR, 'tr.calendar__row'):\n",
    "            # Extract the time from the td with class calendar__time\n",
    "            try:\n",
    "                time_elem = row.find_element(By.CSS_SELECTOR, 'td.calendar__time > div > span')\n",
    "                current_time = time_elem.text.strip() if time_elem.text.strip() else current_time\n",
    "            except:\n",
    "                current_time = current_time  # Keep the last known time\n",
    "\n",
    "            # Extract the currency from the td with class calendar__currency\n",
    "            try:\n",
    "                currency_elem = row.find_element(By.CSS_SELECTOR, 'td.calendar__currency > span')\n",
    "                currency = currency_elem.text.strip()\n",
    "            except:\n",
    "                currency = \"\"\n",
    "\n",
    "            # Extract the impact title from the td with class calendar__impact\n",
    "            try:\n",
    "                impact_elem = row.find_element(By.CSS_SELECTOR, 'td.calendar__impact > span')\n",
    "                impact = impact_elem.get_attribute('title').strip()\n",
    "            except:\n",
    "                impact = \"\"\n",
    "\n",
    "            # Extract the event name from the td with class calendar__event\n",
    "            try:\n",
    "                event_elem = row.find_element(By.CSS_SELECTOR, 'td.calendar__event > div > span')\n",
    "                event_name = event_elem.text.strip()\n",
    "            except:\n",
    "                event_name = \"\"\n",
    "\n",
    "            # Extract the actual value from the td with class calendar__actual\n",
    "            try:\n",
    "                actual_elem = row.find_element(By.CSS_SELECTOR, 'td.calendar__actual > span')\n",
    "                actual = actual_elem.text.strip()\n",
    "            except:\n",
    "                actual = \"\"\n",
    "\n",
    "            # Extract the forecast value from the td with class calendar__forecast\n",
    "            try:\n",
    "                forecast_elem = row.find_element(By.CSS_SELECTOR, 'td.calendar__forecast > span')\n",
    "                forecast = forecast_elem.text.strip()\n",
    "            except:\n",
    "                forecast = \"\"\n",
    "\n",
    "            # Extract the previous value from the td with class calendar__previous\n",
    "            try:\n",
    "                previous_elem = row.find_element(By.CSS_SELECTOR, 'td.calendar__previous > span')\n",
    "                previous = previous_elem.text.strip()\n",
    "            except:\n",
    "                previous = \"\"\n",
    "\n",
    "            # Apply filters and add to the list if matching criteria\n",
    "            if currency in desired_currencies and impact in desired_impacts:\n",
    "                event_data = [formatted_date, current_time, currency, impact, event_name, actual, forecast, previous]\n",
    "                events.append(event_data)\n",
    "                all_events.append(event_data)\n",
    "        \n",
    "        # Cache the events for this date in Redis\n",
    "        if events:\n",
    "            redis_client.set(date, pickle.dumps(events))\n",
    "\n",
    "# Close the browser\n",
    "def close_driver():\n",
    "    driver.quit()\n",
    "\n",
    "# Print the table\n",
    "def print_collected_data():\n",
    "    df_events = pd.DataFrame(all_events, columns=[\"Date\", \"Time\", \"Currency\", \"Impact\", \"Event\", \"Actual\", \"Forecast\", \"Previous\"])\n",
    "    print(tabulate(df_events, headers=\"keys\", tablefmt=\"grid\"))\n",
    "\n",
    "# Main function\n",
    "def collect_forex_data(start_date, end_date):\n",
    "    fetch_data_for_date_range(start_date, end_date)\n",
    "    close_driver()\n",
    "    print_collected_data()\n",
    "\n",
    "# Execute\n",
    "collect_forex_data(start_date, end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da41efd-fcc6-4d9b-84fc-c4d23491f6c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching data for AUDUSD (06 Jan 2021 to 05 Feb 2021)...\n",
      "2024-08-26 11:23:16,629 - findatapy.market.ioengine - INFO - Cache not existent for MarketDataRequest_1012__abstract_curve_key-None__arcticdb_dict-{'prune_previous_versions': False, 'write_style': 'write', 'force_create_library': False, 'allow_on_disk_filter': True, 'query_builder': None}__as_of-None__base_depos_currencies-EUR_GBP_AUD_NZD_USD_CAD_CHF_NOK_SEK_JPY__base_depos_tenor-ON_TN_SN_1W_2W_3W_1M_2M_3M_4M_6M_9M_1Y_2Y_3Y_5Y__category-fx__category_key-backtest_fx_dukascopy_tick_NYC_AUDUSD__cut-NYC__data_engine-None__data_source-dukascopy__environment-backtest__expiry_date-NaT__fields-bid_ask__finish_date-2021-02-05 00:00:00__freeform_md_request-{}__freq-tick__freq_mult-1__fx_forwards_tenor-ON_TN_SN_1W_2W_3W_1M_2M_3M_4M_6M_9M_1Y_2Y_3Y_5Y__fx_vol_part-V_25R_10R_25B_10B__fx_vol_tenor-ON_1W_2W_3W_1M_2M_3M_4M_6M_9M_1Y_2Y_3Y_5Y__gran_freq-None__list_threads-1__old_tickers-AUDUSD__pretransformation-None__push_to_cache-True__resample-None__resample_how-last__split_request_chunks-0__start_date-2021-01-06 00:00:00__tickers-AUDUSD__trade_side-trade__vendor_fields-None__vendor_tickers-None__vintage_as_index-None_df in Redis: 'NoneType' object has no attribute 'read'\n",
      "2024-08-26 11:23:16,633 - findatapy.market.datavendorweb - INFO - Request Dukascopy data\n",
      "2024-08-26 11:23:16,633 - findatapy.market.datavendorweb - INFO - About to download from Dukascopy... for AUDUSD\n",
      "2024-08-26 11:23:16,661 - findatapy.market.datavendorweb - INFO - Downloading... 2021-01-06 00:00:00 https://www.dukascopy.com/datafeed/AUDUSD/2021/00/06/00h_ticks.bi5\n",
      "2024-08-26 11:23:26,625 - findatapy.market.datavendorweb - INFO - Downloading... 2021-01-07 00:00:00 https://www.dukascopy.com/datafeed/AUDUSD/2021/00/07/00h_ticks.bi5\n",
      "2024-08-26 11:23:34,890 - findatapy.market.datavendorweb - INFO - Downloading... 2021-01-08 00:00:00 https://www.dukascopy.com/datafeed/AUDUSD/2021/00/08/00h_ticks.bi5\n",
      "2024-08-26 11:23:43,222 - findatapy.market.datavendorweb - INFO - Downloading... 2021-01-09 00:00:00 https://www.dukascopy.com/datafeed/AUDUSD/2021/00/09/00h_ticks.bi5\n",
      "2024-08-26 11:23:49,188 - findatapy.market.datavendorweb - INFO - Downloading... 2021-01-10 00:00:00 https://www.dukascopy.com/datafeed/AUDUSD/2021/00/10/00h_ticks.bi5\n",
      "2024-08-26 11:23:55,095 - findatapy.market.datavendorweb - INFO - Downloading... 2021-01-11 00:00:00 https://www.dukascopy.com/datafeed/AUDUSD/2021/00/11/00h_ticks.bi5\n",
      "2024-08-26 11:24:04,987 - findatapy.market.datavendorweb - INFO - Downloading... 2021-01-12 00:00:00 https://www.dukascopy.com/datafeed/AUDUSD/2021/00/12/00h_ticks.bi5\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Market object\n",
    "market = Market(market_data_generator=MarketDataGenerator())\n",
    "\n",
    "# Timeframe choice\n",
    "timeframe_choice = 'tick'  # Dukascopy supports only tick data\n",
    "\n",
    "# Function to fetch and analyze data with caching\n",
    "def fetch_and_analyze(timeframe_choice, start_date, end_date, fetch_one_month_prior=False):\n",
    "    if fetch_one_month_prior:\n",
    "        start_date = end_date - timedelta(days=30)\n",
    "\n",
    "    start_date_str = start_date.strftime('%d %b %Y')\n",
    "    end_date_str = end_date.strftime('%d %b %Y')\n",
    "    \n",
    "    # Fetch data from Dukascopy with caching\n",
    "    data_frames = []\n",
    "    for pair in forex_pairs:\n",
    "        print(f\"Fetching data for {pair} ({start_date_str} to {end_date_str})...\")\n",
    "        \n",
    "        md_request = MarketDataRequest(\n",
    "            start_date=start_date,\n",
    "            finish_date=end_date,\n",
    "            category='fx',\n",
    "            fields=['bid', 'ask'],\n",
    "            freq=timeframe_choice,\n",
    "            data_source='dukascopy',\n",
    "            tickers=[pair],\n",
    "            cache_algo='cache_algo_return',  # Use cache algorithm for fetching and saving data\n",
    "            cut='NYC'  # Adjust time cut to match trading hours\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            # Fetch data and cache it\n",
    "            df = market.fetch_market(md_request)\n",
    "            \n",
    "            if df is not None and f'{pair}.bid' in df.columns and f'{pair}.ask' in df.columns:\n",
    "                df['price'] = (df[f'{pair}.bid'] + df[f'{pair}.ask']) / 2  # Mid price\n",
    "                df = df[['price']].resample(new_timeframe).mean()\n",
    "            else:\n",
    "                print(f\"Warning: No valid data for {pair}\")\n",
    "                continue\n",
    "            \n",
    "            data_frames.append(df)\n",
    "\n",
    "            # Retrieve from cache\n",
    "            cached_df = market.fetch_market(md_request)\n",
    "            if cached_df is not None:\n",
    "                print(f\"Data for {pair} retrieved from cache ({start_date_str} to {end_date_str})\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching data for {pair}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Combine all forex pairs into a single DataFrame\n",
    "    if data_frames:\n",
    "        combined_df = pd.concat(data_frames, axis=1)\n",
    "        combined_df.columns = forex_pairs\n",
    "        combined_df.dropna(inplace=True)\n",
    "        \n",
    "        # Calculate correlation matrix\n",
    "        correlation_matrix = combined_df.corr()\n",
    "        \n",
    "        # Plot correlation matrix\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "        sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='RdYlGn', vmin=-1, vmax=1, square=True)\n",
    "        plt.title(f'Correlation Matrix\\n{start_date_str} to {end_date_str} ({new_timeframe} timeframe)')\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No valid data found for the selected forex pairs.\")\n",
    "\n",
    "# Execute with the flag for extended data range\n",
    "fetch_and_analyze(timeframe_choice, start_date, end_date, fetch_one_month_prior=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38915ea6-41cc-41c4-b5ec-2370d3845367",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
